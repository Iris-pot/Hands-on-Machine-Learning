# 分类任务

[TOC]



## 1.性能评估

（1）混淆矩阵

对分类器来说，一个好得多的性能评估指标是**混淆矩阵**。大体思路是：输出类别A被分类成类 别B的次数。

混淆矩阵中的每一行表示一个实际的类,	而每一列表示一个预测的类。

一个完美的分类器将只有真反例和真正例，所以混淆矩阵的非零值仅在其主对角线（左上至右下）。

混淆矩阵可以提供很多信息。有时候你会想要更加简明的指标，具体如下。

①准确率（precision）：正例预测的精度。真正例样本数/预测为正例的样本数

②召回率：真正例样本数/（真正例+假反例） = 真正例样本数/训练集正例样本数

③F1：F1值是准确率和召回率的调和平均。调和平均会给小的值更大的权重。所以，要想分类器得到一个高的F1值，需要召回率和准确率同时高。

（2）ROC曲线

是真正例率（true	positive	rate，另一个名 字叫做召回率）对假正例率（false	positive	rate,	FPR）的曲线。

横坐标是假正例率，纵坐标是真正例率。TPR = recall ，FPR = 1 - TNR.

经验：优先使用PR曲线当正例很少，或者当你关注假正例多于假反例的时候。其他情况使用ROC曲线。

## 2.多分类问题

随机森林分类器、朴素贝叶斯分类器可以直接处理多类分类问题。

SVM分类器、线性分类器则是严格的二分类器。然后，有许多策略可以用二分类器去执行多类分类：

“一对所有”（OvA）策略：训练n个二分类器，选出决策分数最高的那个分类器；

做“一对一”（OvO） 策略：对每一对数字都训练一个二分类器，如果有	N	个类。你需要训练	N*(N-1)/2	个分类器。优点：每个分类器只需要在训练集的部分数据上面进行训 练。这部分数据是它所需要区分的那两个类对应的数据。

OvO可以在小的数据集上面可以更多地训练，较之于巨大的数据集而言。但是， 对于大部分的二分类器来说，OvA是更好的选择。



# 线性模型

目的：加深对机器学习模型内部的理解，从而选择合适的模型、正确的算法，调参以及误差分析。

## 1.线性回归

训练一个模型指的是设置模型的参数使得这个模型在训练集的表现较好。

为此，我们首先需要找到一个衡量模型好坏的评定方 法。在回归模型上，最常见的评定标准是均方根误差（RMSE，详见公 式	2-1）。因此，为了训练一个线性回归模型，你需要找到一个值，它使得均方根误差（标 准误差）达到最小值。

（1）正规方程：

为了找到最小化损失函数的系数值，可以采用公式解，换句话说，就是可以通过解正规方程直接得到最后的结果。

缺点：当特征的个数较大的时候（例如：特征数量为	100000），正规方程求解将会非常慢。

优点：只要有能放得下它的内存空间，它就可以对大规模数据进行训练。

（2）梯度下降：

①基本思想：通过迭代来逐渐调整参数使得损失函数达到最小值。计算误差函数关于参数向量的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值。

函数在一点沿梯度方向的变化率最大。所以沿梯度下降。

②计算方法：具体来说，开始时，需要选定一个随机的参数向量（这个值称为随机初始值），然后逐渐去改进它， 每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛 到一个最小值。

③重要参数：在梯度下降中一个重要的参数是**步长**，**超参数学习率**的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。如果学习率太大，你将跳过最低点，这可能使的算法是**发散**的，函数值变得越来越大，永远不可能找到一个好的答案 。

④梯度陷阱：并不是所有的损失函数看起来都像一个规则的碗，它们可能是洞，山脊，高原和各种不规则的地形，使它们收敛到最小值非常的困难。	

梯度下降的两个主要挑战： 如果随机初始值选在了图像的左侧，则它将**收敛到局部最小值**，这个值要比全局最小值要大。	如果它从右侧开始，那么跨越高原将需要很长时间，如果你早早地结束训练，你将**永远到不了全局最小值。**

**note：线性回归**模型的**均方差损失函数**MSE是一个凸函数，意味着这个损失函数没有局部最小值，仅仅只有一个全局最小值。同时它也是一个斜率不能突变的连续函数。这两个因素导致了一个好的结果：梯度下降可以无限接近全局最小值。

⑤技巧：**归一化**可以加速梯度下降。当不同特征的取值范围相差较大的时，损失函数的图像可能是细长的，这意味着它将需要很长的时间才能够收敛。因此，当我们使用梯度下降的时候，应该确保所有的特征有着相近的尺度范围。

（3）梯度下降的改进

①批量梯度下降：**梯度向量**记为包含了损失函数所有的偏导数（每个模型参数只出现一次）。梯度向量计算过程中每一步计算时都包含了整个训练集，这也是为什么这个算法称为批量梯度下降：每一次训练过程都使用所有的训练数据。

步长：取决于学习率和梯度向量的积。

学习率的选取：网格搜索。

梯度下降的迭代次数：设置一个非常大的迭代次数，但是当梯度向量变得非常小的时候，结束迭代。 非常小指的是：梯度向量小于一个值（称为容差）。这时候可以认为梯度下降几乎已经达到了最小值。

②随机梯度下降：在每一步的梯度计算上只随机选取训练集中的一个样本。算法变得非常快，可以在大规模训练集上使用。

优点：当损失函数很不规则时，随机梯度下降算法**能够跳过局部最小值**。因此，随机梯度下降**在寻找全局最小值上比批量梯度下降表现要好。**

缺点：呈现出更多的不规律性：它到达最小值不是平缓的下降，**损失函数会忽高忽低**，只是在大体上呈下降趋势。随着时间的推移，它会非常的靠近最小值，会一直在这个值附近摆动。因此，当算法停止的时候，**最后的参数还不错，但不是最优值。**

改进方法 ：逐渐降低学习率。开始时，走的每一步较大（这有助于快速前进同时跳过局部最小 值），然后变得越来越小，从而使算法到达全局最小值。这个过程被称为**模拟退火**。

③小批量梯度下降：使用一个随机的小型实例集。

优点：它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用GPU进行运算的时候。

它有可能陷 在局部最小值中

![image-20200610220550768](C:\Users\zmy\AppData\Roaming\Typora\typora-user-images\image-20200610220550768.png)

## 2.多项式回归

## 3. 学习曲线

高阶多项式回归模型会带来严重过拟合，线性模型则欠拟合。介于中间的低阶多项式模型则有较好的泛化能力。那么如何判断过拟合还是欠拟合呢？

首先，我们可以使用**交叉验证**来估计一个模型的泛化能力。如果一个模型在训练集上表现 良好，通过交叉验证指标却得出其泛化能力很差，那么你的模型就是过拟合了。如果在这两 方面都表现不好，那么它就是欠拟合了。

另一种方法是观察**学习曲线**：画出模型在训练集上的表现，同时画出以训练集规模为自变量的训练集函数。为了得到图像，需要在训练集的不同规模子集上进行多次训练。

在统计和机器学习领域有个重要的理论：**一个模型的泛化误差由三个不同误差的和决 定**

1. 偏差：泛化误差的这部分误差是由于错误的假设决定的，也就是模型选择错误。
2. 方差：这部分误差是由于模型对训练数据的微小变化较为敏感，一个多自由度的模型更容易有高的方差，因此会导致模型过拟合。这是由于训练集数据本身变化较大。
3.  噪声：这部分误差是由于数据本身的噪声决定的。降低这部分误差的唯一方法就是进行数据清洗（例如：修复数据源，修复坏的传感器，识别和剔除异常值）。

## 4.正则化

降低模型的过拟合的好方法是正则化这个模型（即限 制它）：模型有越少的自由度，就越难以拟合数据。正则化一个多项式模型，一个简单的方法就是减少多项式的阶数。

对于一个线性模型，正则化的典型实现就是约束模型中参数的权重。接下来我们将介绍三种 不同约束权重的方法：Ridge回归，Lasso回归和Elastic Net。

#### (1)Ridge回归

岭回归（也称为Tikhonov正则化）是线性回归的正则化版：在损失函数上直接加上一个正则项$\alpha\sum_{i=1}^{n}\theta_i^2$。这使得学习算法不仅能够拟合数据，而且能够使模型的参数权重尽量的小。

超参数$\alpha$决定了你想正则化这个模型的强度。如果$\alpha=0$那此时的岭回归便变为了线性回 归。如果$\alpha$非常的大，所有的权重最后都接近于零，最后结果将是一条穿过数据平均值的水平直线。

note:在使用岭回归前，对数据进行放缩（可以使用	StandardScaler	）是非常重要的，算法对 于输入特征的数值尺度（scale）非常敏感。大多数的正则化模型都是这样的。

#### (2) Lasso回归

Lasso回归（是另一 种正则化版的线性回归：就像岭回归那样，它也在损失函数上添加了一个正则化项，但是它使用权重向量的L1范数而不是权重向量L2范数平方的一半。

Lasso回归的一个重要特征是它倾向于完全消除最不重要的特征的权重（即将它们设置为 零）,换句话说，Lasso回归自动的进行特征选 择同时输出一个稀疏模型.

#### (3)ElasticNet

弹性网络介于Ridge回归和Lasso回归之间,它的正则项是Ridge回归和Lasso回归正则项 的简单混合，同时你可以控制它们的混合率r，当r=0时，弹性网络就是Ridge回归，当r=1时，就是Lasso回归。

## 5.逻辑回归

首先估计出属于正类的概率，若大于0.5则判定为正类。

概率预测采用Logit形式。

$\hat{p} = \frac{1}{1+e^{wx}}$

其损失函数为：

![image-20200610225046567](C:\Users\zmy\AppData\Roaming\Typora\typora-user-images\image-20200610225046567.png)

这个损失函数是凸的，所以梯度下降（或任何其他优化算法）一定能够找到全局最小值（如果学习速率不是太大，并且你等待足够长的时间）。

## 6.Softmax回归

Logistic回归模型可以直接推广到支持多类别分类，不必组合和训练多个二分类器，其称为**Softmax回归**或多类别Logistic回归。 

当给定一个实例时，Softmax回归模型首先计算第K类的分数，然 后将分数应用在Softmax函数（也称为归一化指数）上，估计出每类的概率。，Softmax	回归分类器将估计概率最高（它只是得分最高的类）的 那类作为预测结果。

note: Softmax回归分类器一次只能预测一个类（即它是多类的，但不是多输出的），因此它 只能用于判断互斥的类别，如不同类型的植物。	你不能用它来识别一张照片中的多个 人。













