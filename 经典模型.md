# 经典模型

## 1.支持向量机

特点：SVM分类器的判定边界实线，不仅能分开两种类别，而且还尽可能地远离了最靠近的训练数据点。它在两种类别之间保持了一条尽可能宽敞的街道，其被称为**最大间隔分类。**

添加更多的样本点在最大间隔分类外并不会影响到判定边界，因为判定边界是由位于最大间隔分类的样本点确定的，这些样本点被称为**“支持向量”**。

note：SVM对特征缩放比较敏感。

#### （1）硬间隔和软间隔

①硬间隔分类：如果我们严格地规定所有的数据都不在**最大间隔分类**上，都在正确的两边，称为硬间隔分类，硬间隔分类有两个问题，第一，**只对线性可分的数据起作用**，第二，**对异常点敏感**。

②软间隔分类：为了避免上述的问题，我们更倾向于使用更加软性的模型。目的在保持**最大间隔分类尽可能大和避免间隔违规**（例如：数据点出现在最大间隔分类中央或者甚至在错误的一边）之间找到一个良好的平 衡。这就是**软间隔分类**。

使用超参数C（惩罚系数）来控制这种平衡：较小 的C会导致更宽的**最大间隔分类，但更多的间隔违规**.

note:如果你的	SVM	模型过拟合，你可以尝试通过减小超参数	C	去调整。

#### （2）非线性支持向量机

尽管线性	SVM	分类器在许多案例上表现得出乎意料的好，但是很多数据集并不是线性可分 的。**一种处理非线性数据集方法是增加更多的特征**，例如**多项式特征**（正如你在第4章所做的 那样）；在某些情况下可以变成线性可分的数据。

①**添加多项式特征**很容易实现，不仅仅在	SVM，在各种机器学习算法都有不错的表现，但是低次数的多项式不能处理非常复杂的数据集，而高次数的多项式却产生了大量的特征，会使模 型变得慢。

当你使用	SVM	时，你可以运用一个被称为**“核技巧”（kernel	trick）**的神奇数学技巧。它可以取得就像你添加了许多多项式，甚至有高次数的多项式，一样好的结果。所以不 会大量特征导致的组合爆炸，因为你并没有增加任何特征。这个技巧可以用	SVC	类来实现。 

②另一种解决非线性问题的方法是**使用相似函数（similarity	funtion）**计算每个样本与特定地标 （landmark）的相似度。

就像多项式特征法一样，相似特征法对各种机器学习算法同样也有不错的表现。但是在所有 额外特征上的计算成本可能很高，特别是在大规模的训练集上。然而，“核”	技巧再一次显现 了它在	SVM	上的神奇之处：**高斯核让你可以获得同样好的结果成为可能，就像你在相似特征 法添加了许多相似特征一样，但事实上，你并不需要在RBF添加它们。**

γ是可调整的超参数：如果你的模型过拟 合，你应该减小	γ	值，若欠拟合，则增大γ。

③还有其他的核函数，但很少使用。例如，一些核函数是专门用于特定的数据结构。在对文本文档或者	DNA	序列进行分类时，有时会使用**字符串核**（String	kernels）（例如，使用	SSK 核（string	subsequence	kernel）或者**基于编辑距离（Levenshtein	distance）的核函数**）。

④这么多可供选择的核函数，你如何决定使用哪一个？一般来说，你应该**先尝试线性核函 数**（记住	LinearSVC	比	SVC(kernel="linear")	要快得多），尤其是当训练集很大或者有 大量的特征的情况下。**如果训练集不太大，你也可以尝试高斯径向基核（Gaussian	RBF Kernel），它在大多数情况下都很有效**。如果你有空闲的时间和计算能力，你**还可以使 用交叉验证和网格搜索来试验其他的核函数**，特别是有专门用于你的训练集数据结构的 核函数。

#### （3）支持向量机回归

SVM	算法应用广泛：不仅仅支持线性和非线性的分类任务，还支持线 性和非线性的回归任务。技巧在于逆转我们的目标：限制间隔违规的情况下，不是试图在两 个类别之间找到尽可能大的“街道”（即间隔）。SVM	回归任务是限制间隔违规情况下，尽量放置更多的样本在“街道”上。“街道”的宽度由超参数	ϵ	控制。

处理非线性回归任务，你可以使用核化的	SVM	模型。